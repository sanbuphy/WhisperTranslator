{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEAvBF4zgXX4"
      },
      "source": [
        "# WhisperTranslator\n",
        "\n",
        "N46Whisper is a Google Colab notebook application for streamlined video subtitle file generation.The original purpose of the project was to improve the productivity of Nogizaka46 (and Sakamichi groups) subbers. However, it can also be used to create subtitles in general.The application could significantly reduce the labour and time costs of sub-groups or individual subbers. However, despite its impressive performance, the Whisper model, AI translation and the application itself are not without limitations.\n",
        "\n",
        "\n",
        "WhisperTranslator 是基于 [N46Whisper](https://github.com/Ayanaminn/N46Whisper) 的应用。开发初衷旨在提高各类外文视频的转录、翻译、总结效率。\n",
        "\n",
        "此应用基于AI语音识别模型 [Whisper](https://github.com/openai/whisper)的优化部署 [faster-whisper](https://github.com/guillaumekln/faster-whisper).\n",
        "\n",
        "应用输出文件为ass或srt格式，内置指定字幕组的字幕格式，可直接导入 [Aegisub](https://github.com/Aegisub/Aegisub) 进行后续翻译及时间轴校正。你可以根据选项决定是否启动全文摘录和总结。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ2x8S7RMF9i"
      },
      "source": [
        "## 更新/What's Latest：\n",
        "\n",
        "2024.2.20:\n",
        "* release初版，提供转录和输出为分割文章。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 环境安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 首先需要安装pytorch\n",
        "# https://pytorch.org/get-started/locally/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install ffmpeg\n",
        "! pip install pysubs2\n",
        "! pip install faster-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 基础配置\n",
        "\n",
        "你只需要在这里修改配置，之后只需要一路执行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gLcsoJy5BIcW"
      },
      "outputs": [],
      "source": [
        "# 基础参数设置\n",
        "from typing import Optional\n",
        "work_dir:str  = '' #  需要处理的视频放在的文件夹\n",
        "export_dir:str  = './temp' # 结果导出的文件夹\n",
        "file_type:str = \"audio\"  # @param [\"audio\",\"video\"]\n",
        "language:str = \"zh\"  # @param [\"zh\",\"en\",\"jp\"]\n",
        "model_size:str = \"large-v2\"  # @param [\"base\",\"small\",\"medium\", \"large-v1\",\"large-v2\",\"large-v3\"] 推荐v2，也可以试试看v3\n",
        "initial_prompt:Optional[str] = '这个是一个会议对话' # 用于标注这个对话的类型，让结果更准确，比如：'这是一段会议'，'简体中文'\n",
        "export_srt:str = \"No\"  # @param [\"No\",\"Yes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nw72_bK3AS1d"
      },
      "outputs": [],
      "source": [
        "# 其他选项/Advanced settings（可以不管）\n",
        "\n",
        "# 将存在空格的单行文本分割为多行（多句）。分割后的若干行均临时采用相同时间戳，且添加了adjust_required标记提示调整时间戳避免叠轴\n",
        "# 普通分割（Modest): 当空格后的文本长度超过5个字符，则另起一行\n",
        "# 全部分割（Aggressive): 只要遇到空格即另起一行\n",
        "is_split:str = \"No\"  # @param [\"No\",\"Yes\"]\n",
        "split_method:str = \"Modest\"  # @param [\"Modest\",\"Aggressive\"]\n",
        "# 字幕格式（暂时默认default）\n",
        "sub_style:str = \"default\"\n",
        "\n",
        "# 使用VAD过滤/Use VAD filter\n",
        "# 使用[Silero VAD model](https://github.com/snakers4/silero-vad)以检测并过滤音频中的无声段落（推荐小语种使用）\n",
        "# 注意】使用VAD filter有优点亦有缺点，请用户自行根据音频内容决定是否启用. [关于VAD filter](https://github.com/Ayanaminn/N46Whisper/blob/main/FAQ.md)\n",
        "is_vad_filter:str = \"False\" # @param [\"True\", \"False\"]\n",
        "\n",
        "# 设置Beam Size\n",
        "# Beam Size数值越高，在识别时探索的路径越多，这在一定范围内可以帮助提高识别准确性，但是相对的VRAM使用也会更高. 同时，Beam Size在超过5-10后有可能降低精确性，详情请见https://arxiv.org/pdf/2204.05424.pdf\n",
        "# 默认设置为 5\n",
        "set_beam_size:int = 5 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 运行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'work_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m my_root_name \u001b[38;5;241m=\u001b[39m \u001b[43mwork_dir\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m media_names \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, d_names, f_names \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(work_dir):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'work_dir' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pprint\n",
        "\n",
        "my_root_name = work_dir.split('/')[-1]\n",
        "media_names = []\n",
        "for root, d_names, f_names in os.walk(work_dir):\n",
        "    folders = root.split('/')\n",
        "    for folder in folders:\n",
        "        if folder.startswith('.'):\n",
        "            continue\n",
        "    for d_name in d_names:\n",
        "        if d_name.startswith('.'):\n",
        "            d_names.remove(d_name)\n",
        "    for f_name in f_names:\n",
        "        # if f_name.startswith('.'):\n",
        "        #     f_names.remove(f_name)\n",
        "        # only add media files\n",
        "        if f_name.lower().endswith(('mp3','m4a','flac','aac','wav','mp4','mkv','ts','flv')):\n",
        "            media_names.append(f_name)\n",
        "\n",
        "if not os.path.exists(export_dir):\n",
        "    os.makedirs(export_dir)\n",
        "\n",
        "print(\"待处理文件数：\",len(media_names))\n",
        "print(\"待处理文件：\")\n",
        "pprint.pprint(media_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z0igG7ruI-7q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# hf环境变量（可无视）\n",
        "import os\n",
        "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "os.environ['HF_HOME'] = './hf-cache'\n",
        "\n",
        "from faster_whisper import WhisperModel\n",
        "import os\n",
        "import ffmpeg\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import pysubs2\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def split_text(text, max_word_count):\n",
        "    def count_words(text):\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        return len(words)\n",
        "    sentences = re.split(r'(?<=[,.])\\s', text)  # 按照逗号和句号分割文本\n",
        "    new_paragraphs = []\n",
        "    current_paragraph = ''\n",
        "    current_word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_word_count = count_words(sentence)\n",
        "        if current_word_count + sentence_word_count <= max_word_count:\n",
        "            current_paragraph += sentence + ' '\n",
        "            current_word_count += sentence_word_count\n",
        "        else:\n",
        "            if current_word_count > 0:\n",
        "                new_paragraphs.append(current_paragraph.strip())\n",
        "            current_paragraph = sentence + ' '\n",
        "            current_word_count = sentence_word_count\n",
        "\n",
        "    if current_paragraph != '':\n",
        "        new_paragraphs.append(current_paragraph.strip())\n",
        "\n",
        "    return new_paragraphs\n",
        "\n",
        "print('加载模型 Loading model...')\n",
        "torch.cuda.empty_cache()\n",
        "model = WhisperModel(model_size)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "file_names = media_names\n",
        "file_basenames = []\n",
        "for i in range(len(file_names)):\n",
        "    file_basenames.append(Path(file_names[i]).stem)\n",
        "output_dir = Path(export_dir).parent.resolve()\n",
        "\n",
        "for i in range(len(file_names)):\n",
        "  file_name = file_names[i]\n",
        "  #Transcribe\n",
        "  file_basename = file_basenames[i]\n",
        "  if file_type == \"video\":\n",
        "    print('提取音频中 Extracting audio from video file...')\n",
        "    os.system(f'ffmpeg -i {file_name} -f mp3 -ab 192000 -vn {file_basename}.mp3')\n",
        "    print('提取完毕 Done.')\n",
        "\n",
        "  tic = time.time()\n",
        "  print('识别中 Transcribe in progress...')\n",
        "\n",
        "  segments, info = model.transcribe(audio = f'{Path(work_dir) / file_name}',\n",
        "                                        beam_size=set_beam_size,\n",
        "                                        language=language,\n",
        "                                        vad_filter=is_vad_filter,\n",
        "                                        initial_prompt=initial_prompt,\n",
        "                                        vad_parameters=dict(min_silence_duration_ms=1000))\n",
        "\n",
        "  # segments is a generator so the transcription only starts when you iterate over it\n",
        "  # to use pysubs2, the argument must be a segment list-of-dicts\n",
        "  total_duration = round(info.duration, 2)  # Same precision as the Whisper timestamps.\n",
        "  results= []\n",
        "  pure_texts= []\n",
        "\n",
        "  punctuation = [',', '.', '，', '。']\n",
        "\n",
        "  with tqdm(total=total_duration, unit=\" seconds\") as pbar:\n",
        "      for s in segments:\n",
        "        segment_dict = {'start':s.start,'end':s.end,'text':s.text}\n",
        "        results.append(segment_dict)\n",
        "        if not s.text.endswith(tuple(punctuation)):\n",
        "            pure_texts.append(s.text + ',')\n",
        "        else:\n",
        "            pure_texts.append(s.text)\n",
        "        segment_duration = s.end - s.start\n",
        "        pbar.update(segment_duration)\n",
        "  full_text = ''.join(pure_texts)\n",
        "\n",
        "  #Time comsumed\n",
        "  toc = time.time()\n",
        "  print('识别完毕 Done')\n",
        "  print(f'Time consumpution {toc-tic}s')\n",
        "\n",
        "  #Save full text\n",
        "  new_paragraphs = split_text(full_text, max_word_count=200 )\n",
        "  chunk_filename = file_basename + '.txt'\n",
        "  chunk_filename = Path(export_dir) / chunk_filename\n",
        "  with open(chunk_filename, 'w', encoding='utf-8') as file:\n",
        "      for chunk in new_paragraphs:\n",
        "          file.write(chunk + '\\n')\n",
        "\n",
        "  #Save srt\n",
        "  subs = pysubs2.load_from_whisper(results)\n",
        "  srt_filename = file_basename + '.srt'\n",
        "  srt_filename = Path(export_dir) / srt_filename\n",
        "  subs.save(srt_filename)\n",
        "\n",
        "  #Save ass\n",
        "  from srt2ass import srt2ass\n",
        "  ass_filename  = srt2ass(str(srt_filename), sub_style, is_split,split_method)\n",
        "  print('ASS subtitle saved as: ' + ass_filename )\n",
        "\n",
        "  print('第',i+1,'个文件字幕生成完毕/',i+1, 'file(s) was completed!')\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "print('所有字幕生成完毕 All done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 文本翻译\n",
        "\n",
        "待办"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k5n2xrB631JV"
      },
      "outputs": [],
      "source": [
        "# @markdown **AI文本翻译/AI Translation:**\n",
        "# @markdown **</br>**<font size=\"2\"> 此功能允许用户使用AI翻译服务对识别的字幕文件做逐行翻译，并以相同的格式生成双语对照字幕。\n",
        "# @markdown **</br>**阅读项目文档以了解更多。</font>\n",
        "# @markdown **</br>**<font size=\"2\"> This feature allow users to translate previously transcribed subtitle text line by line using AI translation.\n",
        "# @markdown **</br>**Then generate bilingual subtitle files in same sub style.Read documentaion to learn more.</font>\n",
        "\n",
        "# @markdown **</br>**希望在本地使用字幕翻译功能的用户，推荐尝试 [subtitle-translator-electron](https://github.com/gnehs/subtitle-translator-electron)\n",
        "\n",
        "# @markdown **</br><font size=\"3\">Select subtitle file source</br>\n",
        "# @markdown <font size=\"3\">选择字幕文件(使用上一步的转录-use_transcribed/新上传-upload_new）</br>**\n",
        "# @markdown <font size=\"2\">支持SRT与ASS文件\n",
        "sub_source = \"upload_new\"  # @param [\"use_transcribed\",\"upload_new\"]\n",
        "\n",
        "# @markdown **chatGPT:**\n",
        "# @markdown **</br>**<font size=\"2\"> 要使用chatGPT翻译，请填入你自己的OpenAI API Key，目标语言，输出类型，然后执行单元格。</font>\n",
        "# @markdown **</br>**<font size=\"2\"> Please input your own OpenAI API Key, then execute this cell.</font>\n",
        "# @markdown **</br>**<font size=\"2\">【注意】 免费的API对速度有所限制，需要较长时间，用户可以自行考虑付费方案。</font>\n",
        "# @markdown **</br>**<font size=\"2\">【Note】There are limitaions on usage for free API, consider paid plan to speed up.</font>\n",
        "openai_key = '' # @param {type:\"string\"}\n",
        "target_language = 'zh-hans'# @param [\"zh-hans\",\"english\"]\n",
        "prompt = \"You are a language expert.Your task is to translate the input subtitle text, sentence by sentence, into the user specified target language.However, please utilize the context to improve the accuracy and quality of translation.Please be aware that the input text could contain typos and grammar mistakes, utilize the context to correct the translation.Please return only translated content and do not include the origin text.Please do not use any punctuation around the returned text.Please do not translate people's name and leave it as original language.\\\"\" # @param {type:\"string\"}\n",
        "temperature = 0.6 #@param {type:\"slider\", min:0, max:1.0, step:0.1}\n",
        "# @markdown <font size=\"4\">Default prompt: </br>\n",
        "# @markdown ```You are a language expert.```</br>\n",
        "# @markdown ```Your task is to translate the input subtitle text, sentence by sentence, into the user specified target language.```</br>\n",
        "# @markdown ```Please utilize the context to improve the accuracy and quality of translation.```</br>\n",
        "# @markdown ```Please be aware that the input text could contain typos and grammar mistakes, utilize the context to correct the translation.```</br>\n",
        "# @markdown ```Please return only translated content and do not include the origin text.```</br>\n",
        "# @markdown ```Please do not use any punctuation around the returned text.```</br>\n",
        "# @markdown ```Please do not translate people's name and leave it as original language.```</br>\n",
        "output_format = \"ass\"  # @param [\"ass\",\"srt\"]\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import codecs\n",
        "import regex as re\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "\n",
        "if sub_source == 'upload_new':\n",
        "  uploaded = files.upload()\n",
        "  sub_name = list(uploaded.keys())[0]\n",
        "  sub_basename = Path(sub_name).stem\n",
        "elif sub_source == 'use_transcribed':\n",
        "  sub_name = file_basenames[0] +'.ass'\n",
        "  sub_basename = file_basenames[0]\n",
        "\n",
        "!pip install openai\n",
        "!pip install pysubs2\n",
        "from openai import OpenAI\n",
        "import pysubs2\n",
        "\n",
        "clear_output()\n",
        "\n",
        "class ChatGPTAPI():\n",
        "    def __init__(self, key, language, prompt, temperature):\n",
        "        self.key = key\n",
        "        # self.keys = itertools.cycle(key.split(\",\"))\n",
        "        self.language = language\n",
        "        self.key_len = len(key.split(\",\"))\n",
        "        self. prompt = prompt\n",
        "        self.temperature = temperature\n",
        "\n",
        "\n",
        "    # def rotate_key(self):\n",
        "    #     openai.api_key = next(self.keys)\n",
        "\n",
        "    def translate(self, text):\n",
        "        # print(text)\n",
        "        # self.rotate_key()\n",
        "        client = OpenAI(\n",
        "            api_key=self.key,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        # english prompt here to save tokens\n",
        "                        \"content\": f'{self.prompt}'\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\":\"user\",\n",
        "                        \"content\": f\"Original text:`{text}`. Target language: {self.language}\"\n",
        "                    }\n",
        "                ],\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "            t_text = (\n",
        "                completion.choices[0].message.content.encode(\"utf8\").decode()\n",
        "            )\n",
        "            total_tokens = completion.usage.total_tokens # include prompt_tokens and completion_tokens\n",
        "        except Exception as e:\n",
        "            # TIME LIMIT for open api , pay to reduce the waiting time\n",
        "            sleep_time = int(60 / self.key_len)\n",
        "            time.sleep(sleep_time)\n",
        "            print(e, f\"will sleep  {sleep_time} seconds\")\n",
        "            # self.rotate_key()\n",
        "            client = OpenAI(\n",
        "            api_key=self.key,\n",
        "            )\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": f'{self.prompt}'\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Original text:`{text}`. Target language: {self.language}\"\n",
        "                    }\n",
        "                ],\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "            t_text = (\n",
        "                completion.choices[0].message.content.encode(\"utf8\").decode()\n",
        "            )\n",
        "        total_tokens = completion.usage.total_tokens\n",
        "        return t_text, total_tokens\n",
        "\n",
        "\n",
        "class SubtitleTranslator():\n",
        "    def __init__(self, sub_src, model, key, language, prompt,temperature):\n",
        "        self.sub_src = sub_src\n",
        "        self.translate_model = model(key, language,prompt,temperature)\n",
        "        self.translations = []\n",
        "        self.total_tokens = 0\n",
        "\n",
        "    def calculate_price(self,num_tokens):\n",
        "        price_per_token = 0.000002 #gpt-3.5-turbo\t$0.002 / 1K tokens\n",
        "        return num_tokens * price_per_token\n",
        "\n",
        "    def translate_by_line(self):\n",
        "        sub_trans = pysubs2.load(self.sub_src)\n",
        "        total_lines = len(sub_trans)\n",
        "        for line in tqdm(sub_trans,total = total_lines):\n",
        "            line_trans, tokens_per_task = self.translate_model.translate(line.text)\n",
        "            line.text += (r'\\N'+ line_trans)\n",
        "            print(line_trans)\n",
        "            self.translations.append(line_trans)\n",
        "            self.total_tokens += tokens_per_task\n",
        "\n",
        "        return sub_trans, self.translations, self.total_tokens\n",
        "\n",
        "\n",
        "clear_output()\n",
        "\n",
        "translate_model = ChatGPTAPI\n",
        "\n",
        "assert translate_model is not None, \"unsupported model\"\n",
        "OPENAI_API_KEY = openai_key\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise Exception(\n",
        "        \"OpenAI API key not provided, please google how to obtain it\"\n",
        "    )\n",
        "# else:\n",
        "#     OPENAI_API_KEY = openai_key\n",
        "\n",
        "t = SubtitleTranslator(\n",
        "    sub_src=sub_name,\n",
        "    model= translate_model,\n",
        "    key = OPENAI_API_KEY,\n",
        "    language=target_language,\n",
        "    prompt=prompt,\n",
        "    temperature=temperature)\n",
        "\n",
        "translation, _, total_token = t.translate_by_line()\n",
        "total_price = t.calculate_price(total_token)\n",
        "#Download ass file\n",
        "\n",
        "if output_format == 'ass':\n",
        "  translation.save(sub_basename + '_translation.ass')\n",
        "  files.download(sub_basename + '_translation.ass')\n",
        "elif output_format == 'srt':\n",
        "  translation.save(sub_basename + '_translation.srt')\n",
        "  files.download(sub_basename + '_translation.srt')\n",
        "\n",
        "\n",
        "\n",
        "print('双语字幕生成完毕 All done!')\n",
        "print(f\"Total number of tokens used: {total_token}\")\n",
        "print(f\"Total price (USD): ${total_price:.4f}\")\n",
        "\n",
        "# @markdown **</br>**<font size='3'>**实验功能的开发亦是为了尝试帮助大家更有效率的制作字幕。但是只有在用户实际使用体验反馈的基础上，此应用才能不断完善，如果您有任何想法，都欢迎以任何方式联系我，提出[issue](https://github.com/Ayanaminn/N46Whisper/issues)或者分享在[讨论区](https://github.com/Ayanaminn/N46Whisper/discussions)。**\n",
        "# @markdown **</br>**<font size='3'>**The efficacy of this application cannot get improved without the feedbacks from everyday users.Please feel free to share your thoughts with me or post it [here](https://github.com/Ayanaminn/N46Whisper/discussions)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 文本总结"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
